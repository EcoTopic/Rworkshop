{
  "hash": "bf4e664ba3a2a1c8d632a5967b12137f",
  "result": {
    "markdown": "---\ntitle: \"Hypothesis testing, data transformation, longitudinal displays\"\nformat:\n  html:\n    toc: true\n---\n\n\n\n## Slides\n[Make slides full screen](slides.qmd)\n\n\n```{=html}\n<iframe class=\"slide-deck\" src=\"slides.html\" height=\"420\" width=\"747\" style=\"border: 1px solid #2e3846;\"></iframe>\n```\n\n```{=html}\n<h2><a href=\"https://raw.githubusercontent.com/elsherbini/durban-data-science-for-biology/main/materials/1-workshop1/6-hypothesis-testing/hands-on.qmd\" download>Download the `.qmd` to do the exercise</a></h2>\n```\n\n\n---\n\n---\nformat:\n html:\n    code-fold: true\n    toc: true\neditor: visual\n---\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse) # loads the packages from the tidyverse suite\nlibrary(pander) # allows to display pretty tables \nlibrary(patchwork) # allows to combine ggplot (see Module 7)\ntheme_set(theme_light())\nset.seed(123) # set the \"random seed\" for reproducibility\n```\n:::\n\n## Hypothesis testing\n\nBefore testing hypotheses on real data, let's develop our intuition on data we simulate ourselves.\n\nFor this first example, let's simulate 3 datasets by sampling 50 values from 2 normal populations with different means but same variance (So two datasets are drawn from population 1 and one is drawn from population 2). Let's use $\\mu_1 = 0$, $\\mu_2 = 2$, and $\\sigma^2 = 4$.\n\n::: {.callout-tip appearance=\"minimal\"}\nTo simulate normally distributed data, check the `rnorm` function by typing `?rnorm` in the console.\n\nWe put the results of our simulations in a variable `X` that will be a \"long\" tibble with the following two columns: `dataset`, `value`.\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nn_samples <- 50\ncommon_sd <- 2 # square root of 4 (we said)\n\nsample_1 <- rnorm(n_samples, mean = 0, sd = common_sd) \nsample_2 <- rnorm(n_samples, mean = 0, sd = common_sd) \nsample_3 <- rnorm(n_samples, mean = 2, sd = common_sd)\n\nX <- \n  bind_rows(\n    tibble(dataset = 1, value = sample_1),\n    tibble(dataset = 2, value = sample_2),\n    tibble(dataset = 3, value = sample_3)\n  ) |> \n  mutate(dataset = dataset |> factor())\n```\n:::\n\nIf we display the top 6 raws, we have:\n\n::: {.cell}\n\n```{.r .cell-code}\n# the head function takes the top x rows of a table\n# by default, the first 6 rows\n# pander just prints the result in a pretty way\nX |> head() |> pander()\n```\n\n::: {.cell-output-display}\n-------------------\n dataset    value  \n--------- ---------\n    1      -1.121  \n\n    1      -0.4604 \n\n    1       3.117  \n\n    1       0.141  \n\n    1      0.2586  \n\n    1       3.43   \n-------------------\n:::\n:::\n\nand the bottom 6 raws are:\n\n::: {.cell}\n\n```{.r .cell-code}\n# tail prints the last 6 rows\nX |> tail() |> pander()\n```\n\n::: {.cell-output-display}\n-------------------\n dataset    value  \n--------- ---------\n    3      -1.203  \n\n    3      0.9382  \n\n    3      -0.9235 \n\n    3       3.376  \n\n    3        6.2   \n\n    3      -0.5741 \n-------------------\n:::\n:::\n\nTo validate that we've simulated our data properly, let's compute the mean and variance of our data and display a histogram for each dataset.\n\n::: {.cell}\n\n```{.r .cell-code}\nX %>% \n  group_by(dataset) %>% \n  summarize(mean = mean(value), var = var(value)) |> \n  pander()\n```\n\n::: {.cell-output-display}\n---------------------------\n dataset    mean      var  \n--------- --------- -------\n    1      0.06881   3.429 \n\n    2      0.2928    3.279 \n\n    3       1.492    3.915 \n---------------------------\n:::\n:::\n\nDo these values make sense?\n\nWe can display the distributions we just simulated using `ggplot` and `geom_histogram`.\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(X, aes(x = value, fill = dataset)) +\n  geom_histogram(bins = 50) +\n  facet_grid(dataset ~ ., labeller = label_both) +\n  guides(fill = \"none\") + # we don't really need a fill legend \n  theme(strip.text.y = element_text(angle = 0, hjust = 0)) # sometimes it is nicer to rotate the panel labels so they are horizontal and more easily readable\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-6-1.png){width=672}\n:::\n:::\n\nVisually, it looks like the **means** of datasets 1 and 2 are similar, but the **mean** of dataset 3 is larger (expected based on how we simulated the data). However, because there is large variability in the data and a lot of overlap between the values of these datasets, we want to make sure that what we think we observe is not due to chance.\n\nThis what statistical tests help us do. Feel free to review the workshop slides if you need a reminder of what statistical tests are.\n\nHere, we want to do a test on the **means** of the datasets and test if the means of datasets 2 or 3 are different from the mean of dataset 1.\n\n### $t$-tests\n\nBecause we have more than 40 samples in each dataset, we can use a $t$-test.\n\nIn `R`, $t$-tests can be done with the `t.test` function. Note that there are several ways to use the `t.test` function and several options we need to be careful about. To read more about these options, type `?t.test` in your console.\n\nSpecifically, we need to be careful about specifying what our Null and alternative hypotheses are.\n\nThis is specified using the `alternative` option of the `t.test` function.\n\nThe option corresponding to\n\n```{=tex}\n\\begin{align*}\nH_0:& \\ \\mu_1 = \\mu_2 \\\\\nH_a:& \\ \\mu_1 \\neq \\mu_2\n\\end{align*}\n```\nis `alternative = \"two-sided\"` which is the default option of `t.test` (that is, if we don't specify the `alternative`, the function automatically assumes that we want to perform a \"two-sided\" test). It is `\"two-sided\"` because $\\mu_2$ can be smaller OR larger in the alternative hypothesis.\n\nLet's say that we do not have any *a priori* on the alternative and perform a two-sided test on the means of datasets 1 and 2.\\\n\n::: {.cell}\n\n```{.r .cell-code}\nt.test(value ~ dataset, data = X |> filter(dataset %in% 1:2))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\n\tWelch Two Sample t-test\n\ndata:  value by dataset\nt = -0.61157, df = 97.951, p-value = 0.5422\nalternative hypothesis: true difference in means between group 1 and group 2 is not equal to 0\n95 percent confidence interval:\n -0.9508970  0.5028781\nsample estimates:\nmean in group 1 mean in group 2 \n      0.0688071       0.2928165 \n```\n:::\n\n```{.r .cell-code}\n# this is the same as :\n# t.test(x = X$value[X$dataset == 1], y = X$value[X$dataset == 2])\n```\n:::\n\nWe see that the probability to observe the $t$ value is quite large. So, we do NOT reject the null hypothesis that the two means are the same.\n\nNow, if we do this test comparing the means of datasets 1 and **3**, what do we get? What do you conclude?\n\n::: {.cell}\n\n```{.r .cell-code}\nt.test(value ~ dataset, data = X |> filter(dataset %in% c(1,3)))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\n\tWelch Two Sample t-test\n\ndata:  value by dataset\nt = -3.714, df = 97.572, p-value = 0.0003399\nalternative hypothesis: true difference in means between group 1 and group 3 is not equal to 0\n95 percent confidence interval:\n -2.1839829 -0.6628012\nsample estimates:\nmean in group 1 mean in group 3 \n      0.0688071       1.4921991 \n```\n:::\n:::\n\nAs explained above, the tests we have done so far are \"two-sided\". That means that the null hypothesis is that the two means are the same, and the alternative is that they are different.\n\n$$\nH_0: \\mu_1 = \\mu_2\n$$\n\n$$\nH_a: \\mu_1 \\neq \\mu_2\n$$\n\nBut sometimes, we have an *a priori* that the alternative is that one of the two means is larger. This is where we need a \"one-sided\" test.\n\nWe do this in `R` with the `alternative` option of the `t.test` function.\n\nRemember: to obtain the documentation about the `t.test` function, you can type `?t.test` in the console.\n\nHow do we need to change the `alternative` option to perform the test corresponding to this pair of hypotheses ?\n\n```{=tex}\n\\begin{align*}\nH_0:&\\  \\mu_1 \\geq \\mu_3 \\\\\nH_a:&\\  \\mu_1 < \\mu_3 ( \\iff \\mu_1 - \\mu_3 < 0 )\n\\end{align*}\n```\n::: {.cell}\n\n```{.r .cell-code}\nt.test(value ~ dataset, data = X |> filter(dataset %in% c(1,3)),\n       alternative = \"less\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\n\tWelch Two Sample t-test\n\ndata:  value by dataset\nt = -3.714, df = 97.572, p-value = 0.0001699\nalternative hypothesis: true difference in means between group 1 and group 3 is less than 0\n95 percent confidence interval:\n       -Inf -0.7869575\nsample estimates:\nmean in group 1 mean in group 3 \n      0.0688071       1.4921991 \n```\n:::\n:::\n\nHow do the $t$ and $p$ values compare in the two-sided *vs* one-sided test?\\\nWhy?\n\n#### QQ-plots: checking compatibility with a (normal) distribution\n\nRemember that the $t$-test requires, for small sample sizes, for the data to be drawn from a normal distribution. Usually, with real data, we don't always know what distribution the data are samples from.\n\nWe can always check that our data is *compatible* with a normal distribution by performing a QQ-plot:\n\n::: {.cell}\n\n```{.r .cell-code}\nexample_data <- \n  tibble(norm = rnorm(20), lnorm = rlnorm(20))\n# just like `rnorm` samples from a normal distribution, \n# `runif` samples from a uniform distribution\n\ng_norm <- \n  ggplot(example_data, aes(sample = norm)) +\n  geom_qq() +\n  geom_qq_line() +\n  ggtitle(\"Sampled from a\\nnormal distribution\")\n\n\ng_lnorm <- \n  ggplot(example_data, aes(sample = lnorm)) +\n  geom_qq() +\n  geom_qq_line() +\n  ggtitle(\"Sampled from a\\n*log*normal distribution\")\n\ng_norm + g_lnorm\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-10-1.png){width=672}\n:::\n:::\n\nWe see that, on the left plot, the dots are close to the line, and on the right plot, some dots are very far from the line. These dots far from the line are a warning that the distribution is probably not normal.\n\n### Impact of sample size on p-values\n\n#### Small dataset\n\nLet's re-do our analysis but decrease the sample size to 10 samples per dataset.\n\n::: {.cell}\n\n```{.r .cell-code}\n# another way to simulate data is to do the following\n# check what the expand_grid and rowwise function do\nX_small_sample <- \n  expand_grid(\n    dataset = (1:3) |> factor(), \n    sample = 1:10\n  ) %>% \n  mutate(\n    pop_true_mean = ifelse(dataset == 3, 2, 0),\n    pop_true_var = 2\n  ) %>% \n  rowwise() %>% \n  mutate(\n    value = \n      rnorm(1, mean = pop_true_mean, sd = sqrt(pop_true_var))\n  ) %>% \n  ungroup()\n\nX_small_sample |> head() |> pander()\n```\n\n::: {.cell-output-display}\n-----------------------------------------------------------\n dataset   sample   pop_true_mean   pop_true_var    value  \n--------- -------- --------------- -------------- ---------\n    1        1            0              2         0.3033  \n\n    1        2            0              2         -0.4592 \n\n    1        3            0              2         0.1338  \n\n    1        4            0              2         -1.266  \n\n    1        5            0              2         -1.854  \n\n    1        6            0              2          2.824  \n-----------------------------------------------------------\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\nggplot(X_small_sample, aes(x = value, fill = dataset)) +\n  geom_histogram(bins = 50) +\n  facet_grid(dataset ~ ., labeller = label_both) +\n  guides(fill = \"none\") + \n  theme(strip.text.y = element_text(angle = 0, hjust = 0)) \n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-12-1.png){width=672}\n:::\n:::\n\nWe can still use a $t$-test because we *know* that our samples are drawn from a Normal distribution.\n\n::: {.cell}\n\n```{.r .cell-code}\nt.test(value ~ dataset, \n       data = X_small_sample |> filter(dataset %in% c(1,3)))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\n\tWelch Two Sample t-test\n\ndata:  value by dataset\nt = -3.2548, df = 17.988, p-value = 0.004402\nalternative hypothesis: true difference in means between group 1 and group 3 is not equal to 0\n95 percent confidence interval:\n -3.4653702 -0.7465168\nsample estimates:\nmean in group 1 mean in group 3 \n      -0.377852        1.728091 \n```\n:::\n:::\n\nWe see that the $p$-value is now much larger because, with the small datasets, we have a lot more uncertainty on the true means of the populations.\n\n#### Large datasets\n\nLet's now redo the same again but with a very large sample size for each dataset. For example N = 1000.\n\n::: {.cell}\n\n```{.r .cell-code}\nX_large_sample <- \n  expand_grid(\n    dataset = (1:3) |> factor(), \n    sample = 1:1000\n    ) %>% \n  mutate(\n    pop_true_mean = ifelse(dataset == 3, 2, 0),\n    pop_true_var = 2\n  ) %>% \n  rowwise() %>% \n  mutate(\n    value = \n      rnorm(1, mean = pop_true_mean, sd = sqrt(pop_true_var))\n  ) %>% \n  ungroup()\n\nX_large_sample |> head() |> pander()\n```\n\n::: {.cell-output-display}\n-----------------------------------------------------------\n dataset   sample   pop_true_mean   pop_true_var    value  \n--------- -------- --------------- -------------- ---------\n    1        1            0              2         -0.8117 \n\n    1        2            0              2          0.874  \n\n    1        3            0              2          1.57   \n\n    1        4            0              2          1.001  \n\n    1        5            0              2         -0.5143 \n\n    1        6            0              2         0.0845  \n-----------------------------------------------------------\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\nggplot(X_large_sample, aes(x = value, fill = dataset)) +\n  geom_histogram(bins = 50) +\n  facet_grid(dataset ~ ., labeller = label_both) +\n  guides(fill = \"none\") + \n  theme(strip.text.y = element_text(angle = 0, hjust = 0)) \n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-15-1.png){width=672}\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nt.test(value ~ dataset, \n       data = X_large_sample |> filter(dataset %in% c(1,3)))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\n\tWelch Two Sample t-test\n\ndata:  value by dataset\nt = -30.732, df = 1997.9, p-value < 2.2e-16\nalternative hypothesis: true difference in means between group 1 and group 3 is not equal to 0\n95 percent confidence interval:\n -2.075287 -1.826308\nsample estimates:\nmean in group 1 mean in group 3 \n     0.03031352      1.98111096 \n```\n:::\n:::\n\nThe $p$-value is as small as it can be.\n\nLet's also re-do the test comparing datasets 1 and 2\n\n::: {.cell}\n\n```{.r .cell-code}\nt.test(value ~ dataset, \n       data = X_large_sample |> filter(dataset %in% c(1,2)))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\n\tWelch Two Sample t-test\n\ndata:  value by dataset\nt = 0.075717, df = 1996.9, p-value = 0.9397\nalternative hypothesis: true difference in means between group 1 and group 2 is not equal to 0\n95 percent confidence interval:\n -0.1186693  0.1282006\nsample estimates:\nmean in group 1 mean in group 2 \n     0.03031352      0.02554787 \n```\n:::\n:::\n\nWe still don't reject the null hypothesis, which is what we expect.\n\n#### Clinical *vs* Statistical significance\n\nHowever, sometimes, we need to be careful with large sample sizes because tiny effects can still lead to very small p-values. However, these tiny effects don't have much but these do not have much *clinical* relevance.\n\nLet's verify that with a difference in means of 0.2\n\n::: {.cell}\n\n```{.r .cell-code}\nt.test(\n  x = rnorm(1000, mean = 0.0, sd = 2),\n  y = rnorm(1000, mean = 0.2, sd = 2)\n)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\n\tWelch Two Sample t-test\n\ndata:  rnorm(1000, mean = 0, sd = 2) and rnorm(1000, mean = 0.2, sd = 2)\nt = -2.18, df = 1998, p-value = 0.02937\nalternative hypothesis: true difference in means is not equal to 0\n95 percent confidence interval:\n -0.36547886 -0.01931448\nsample estimates:\n  mean of x   mean of y \n-0.02210611  0.17029056 \n```\n:::\n:::\n\nThe $p$-value is lower than 1/20, but the effect (*i.e.*, the mean in differences), compared to the standard deviations is very small.\n\nThis is what we are talking about when discussing the \"clinical\" *vs* \"statistical\" significance.\n\n## Data transformation\n\nNow, let's play with the datasets provided on the workshop website. Specifically, we are interested in analyzing the cytokines data.\n\n### Cytokine data exploration\n\nOur first task will be to display the distribution of the \"IL-1$\\beta$\" cytokine.\n\nTo do so, we load the cytokine data and filter for the cytokine of interest.\n\n::: {.cell}\n\n```{.r .cell-code}\n# notice where I had stored my files.\n# Make sure to modify the path to your files\n\nelisa <- \n  read_csv(\n    file = \"data/03_elisa_cytokines_UKZN_workshop_2023.csv\",\n    show_col_types = FALSE\n  )\n\nIL1b <- elisa |> filter(cytokine == \"IL-1b\") \n```\n:::\n\nWe display its distribution with the `geom_histogram` function, and we can use the color (`fill`) option to flag whether the concentration value was within or out of the limits of detection.\n\n::: {.cell}\n\n```{.r .cell-code}\ncytokine_lim_cols <-\n  c(\"within limits\" = \"navy\", \"out of range\" = \"indianred1\")\n\nggplot(IL1b, aes(x = conc, fill = limits)) +\n  geom_histogram(bins = 30) +\n  ggtitle(\"IL-1b concentrations\") +\n  scale_fill_manual(values = cytokine_lim_cols)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-20-1.png){width=672}\n:::\n:::\n\nWhat do we observe?\n\nDo we think that the distribution of IL-1$\\beta$ concentations follow a normal distribution?\n\nLet's still check with a QQ-plot:\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(\n  IL1b, \n  aes(sample = conc)\n) +\n  geom_qq_line() +\n  geom_qq(size = 0.75, alpha = 0.5)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-21-1.png){width=672}\n:::\n:::\n\nLook back at the QQ-plots we did earlier on simulated data. Does this QQ-plot look like one of the simulated one?\n\n### Log-transformation\n\nLet's now repeat the last two displays, using the log of the concentration instead of the concentration itself. Remember, we can use the `mutate` function to add a new variable to our `data.frame`.\n\n::: {.cell}\n\n```{.r .cell-code}\nIL1b <- IL1b |> mutate(logconc = log10(conc))\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(IL1b, aes(x = logconc, fill = limits)) +\n  geom_histogram(bins = 30) +\n  ggtitle(\"IL-1b concentrations (log 10)\") +\n  xlab(\"log10(conc)\") +\n  scale_fill_manual(values = cytokine_lim_cols)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-23-1.png){width=672}\n:::\n:::\n\nNow, the \"within-range\" data is more compatible with a Normal distribution.\n\nLet's check with a QQ-plot:\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(\n  IL1b, \n  aes(sample = logconc)\n) +\n  geom_qq_line() +\n  geom_qq(size = 0.75, alpha = 0.5) \n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-24-1.png){width=672}\n:::\n:::\n\nIt is not perfect, but it is much better.\n\n### Association with BV\n\nNow, we are interested in testing whether a BV (Bacterial Vaginosis) diagnosis is associated with different mean concentrations of IL-1$\\beta$.\n\nRemember, the BV diagnosis is contained in the Clinical Measurements table. So we first need to load that table in `R`.\n\n::: {.cell}\n\n```{.r .cell-code}\nclin <- \n  read_csv(\n    \"data/02_visit_clinical_measurements_UKZN_workshop_2023.csv\",\n    show_col_types = FALSE\n  )\n```\n:::\n\nThe first 6 rows of the clinical data are:\n\n::: {.cell}\n\n```{.r .cell-code}\nclin |> head() |> pander()\n```\n\n::: {.cell-output-display}\n----------------------------------------------------------------\n  pid     time_point     arm     nugent_score   crp_blood   ph  \n-------- ------------ --------- -------------- ----------- -----\n pid_01    baseline    placebo        8           0.44      5.7 \n\n pid_01     week_1     placebo        7           1.66      5.2 \n\n pid_01     week_7     placebo        7           1.44      5.4 \n\n pid_02    baseline    placebo        7           1.55      5.2 \n\n pid_02     week_1     placebo        7           0.75      4.8 \n\n pid_02     week_7     placebo        4           1.17      4.2 \n----------------------------------------------------------------\n:::\n:::\n\nThere are several ways to diagnose BV. One of them is to check whether a person has a Nugent score of 7 or more.\n\nLet's create a new `BV` variable that says whether a person was diagnosed with BV at each visit.\n\n::: {.cell}\n\n```{.r .cell-code}\nclin <- \n  clin |> \n  mutate(BV = ifelse(nugent_score >= 7, \"BV\", \"Healthy\")) \n```\n:::\n\nSince we want to compare the IL-1$\\beta$ concentrations of individuals with or without BV, we need to join the clinical data with the IL-1$\\beta$ concentration data.\n\nHint: to do so, we need a table describing how the sample IDs and participant x visit IDs are linked together - this info is in the \"Sample ID\" table.\n\n::: {.cell}\n\n```{.r .cell-code}\nsample_info <- \n  read_csv(\n    file = \"data/00_sample_ids_UKZN_workshop_2023.csv\",\n    show_col_types = FALSE\n  )\n```\n:::\n\nThe first 6 rows of the sample info table are:\n\n::: {.cell}\n\n```{.r .cell-code}\nsample_info |> head() |> pander()\n```\n\n::: {.cell-output-display}\n---------------------------------------------\n  pid     time_point      arm      sample_id \n-------- ------------ ----------- -----------\n pid_17    baseline    treatment    SAMP001  \n\n pid_22     week_1     treatment    SAMP002  \n\n pid_25     week_1     treatment    SAMP003  \n\n pid_41     week_1     treatment    SAMP004  \n\n pid_44     week_7     treatment    SAMP005  \n\n pid_16     week_1     treatment    SAMP006  \n---------------------------------------------\n:::\n:::\n\nWe see that we can use the `sample_id` column to bind the cytokine concentrations with the `sample_info` table, then bind on the participant and visit ID with the clinical data.\n\n::: {.cell}\n\n```{.r .cell-code}\nIL1b <- \n  IL1b |> \n  left_join(sample_info, by = join_by(sample_id)) |> \n  left_join(clin, by = join_by(pid, time_point, arm))\n```\n:::\n\nNow that our data are joined, let's display the histogram of IL-1$\\beta$ concentrations, including those out-of-range, colored by BV diagnosis.\n\n::: {.cell}\n\n```{.r .cell-code}\nBV_colors <- \n  c(\"BV\" = \"darkgoldenrod1\", \"Healthy\" = \"cornflowerblue\")\n\nggplot(\n  IL1b, #|> filter(limits == \"within limits\"),\n  aes(x = logconc, fill = BV)\n) +\n  geom_histogram(bins = 30, alpha = 0.5, position = \"identity\") +\n  xlab(\"log10(conc)\") +\n  ggtitle(\"IL-1b concentration by BV diagnosis\") +\n  scale_fill_manual(values = BV_colors)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-31-1.png){width=672}\n:::\n:::\n\nFrom this visualization, it looks like the distribution is lower in healthy participants than in participants with BV.\n\nWhat do we think of the \"out-of-range\" values? Should we include them in our analysis?\n\nLet's do a statistical test to see if these differences in concentrations could have happened by chance.\n\nSince our data, once log-transformed, look normal, (with the exclusion of the out-of-range samples) it makes sense to use a $t$-test. We can also check how many samples we have in each group to check if, regardless of the underlying distribution, there are enough samples to use a $t$-test.\n\n::: {.cell}\n\n```{.r .cell-code}\nIL1b |> \n  select(BV) |> \n  table()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nBV\n     BV Healthy \n     46      86 \n```\n:::\n:::\n\nWe have many samples, so we could, regardless of the underlying distribution, use a $t$-test.\n\n::: {.cell}\n\n```{.r .cell-code}\nt.test(logconc ~ BV, data = IL1b)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\n\tWelch Two Sample t-test\n\ndata:  logconc by BV\nt = 6.5449, df = 116.4, p-value = 1.672e-09\nalternative hypothesis: true difference in means between group BV and group Healthy is not equal to 0\n95 percent confidence interval:\n 0.5332052 0.9959381\nsample estimates:\n     mean in group BV mean in group Healthy \n            1.1245273             0.3599557 \n```\n:::\n:::\n\nWhat do you conclude?\n\n## Non-parametric tests\n\nSo far, we could use the $t$-test to make inference on the means of different populations because we either had more than 40 samples in each group or we observed or *knew* that the samples were drawn from a normal distribution.\n\nSo, when our data is small and does not appear to be drawn from a normal distribution, we cannot use the $t$-test that assumes normality of the underlying data or requires large enough sample sizes.\n\nThe $t$-test is part of the family of *parametric* tests because they assume that the underlying data follows a specific distribution which can be characterized by *parameters*. For example, the parameters of a normal distribution are the mean and the variance.\n\nThere are also *non-parametric* tests which makes no assumption on the distribution of the data.\n\n### The Wilcoxon rank-sum test\n\nThe Wilcoxon rank-sum test is a non-parametric test to compare two independent datasets. The null hypothesis is that, for randomly selected values $X$ and $Y$ from two populations, the probability of $X$ being greater than $Y$ is equal to the probability of $Y$ being greater than $X$ (see [Wikipedia](https://en.wikipedia.org/wiki/Mann–Whitney_U_test)), regardless of the distribution $X$ and $Y$ are drawn from.\n\nLet's try the test on our small dataset:\n\n::: {.cell}\n\n```{.r .cell-code}\nwilcox.test(value ~ dataset, \n            data = X_small_sample |> filter(dataset %in% c(1,3)))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\n\tWilcoxon rank sum exact test\n\ndata:  value by dataset\nW = 13, p-value = 0.003886\nalternative hypothesis: true location shift is not equal to 0\n```\n:::\n:::\n\nAnd compare it to the $t$-test (we can because we know this data is drawn from a Normal).\n\n::: {.cell}\n\n```{.r .cell-code}\nt.test(value ~ dataset, \n       data = X_small_sample |> filter(dataset %in% c(1,3)))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\n\tWelch Two Sample t-test\n\ndata:  value by dataset\nt = -3.2548, df = 17.988, p-value = 0.004402\nalternative hypothesis: true difference in means between group 1 and group 3 is not equal to 0\n95 percent confidence interval:\n -3.4653702 -0.7465168\nsample estimates:\nmean in group 1 mean in group 3 \n      -0.377852        1.728091 \n```\n:::\n:::\n\nWe see that both provides small probabilites to observe the data under the null hypothesis.\n\nSo, why not always using a non-parametric test if they work in all situations and not worry about normality of the data?\n\nBecause, non-parametric tests have **less power to detect small effects**.\n\nTo check that, we can rely on simulations.\\\nHere, we simulate 1000 times two small datasets of 10 samples with a relatively small difference between their means (relative to the standard deviation) and perform both a $t$ test and a Wilcoxon rank sum test.\n\nWe then count how many times each test rejected the null (as they should) and see if one of the two tests reject more often.\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(123)\nsimulate_and_run_both_test <- function(){\n  x1 <- rnorm(10, mean = 0, sd = 2)\n  x2 <- rnorm(10, mean = 1.5, sd = 2)\n  \n  ttest <- t.test(x1, x2)\n  wtest <- wilcox.test(x1, x2)\n  tibble(ttest_pvalue = ttest$p.value, wtest_pvalue = wtest$p.value)\n}\n\nsimulation_results <- replicate(1000, simulate_and_run_both_test())\n\napply(simulation_results <= 0.05, 2, mean)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nttest_pvalue wtest_pvalue \n       0.354        0.319 \n```\n:::\n:::\n\nWe see that the $t$-test more frequently detected that the two samples were drawn from distribution with different means/locations.\n\n## Multiple testing\n\n### Simulations\n\nAs we discussed above, the $p$-value of a test is the probability, under the null hypothesis, to observe a value of the test statistics as extreme as the one we observe. Usually, if that probability is less than 0.05 (we have less than one/20 chances to observe that value), we reject the null hypothesis.\n\nSo, if we repeat a test many many times on data generated under the null hypothesis (so we should *not* reject it), we will obtain a small $p$-value a few times.\n\nTo verify this, let's do a simulation experiment and repeat the following procedure a 1000 times: we draw two small datasets from that same population (= the null hypothesis is true) and perform a $t$-test. In theory, we should get 0.05 x 1000 = 50 experiments with a $p$-value smaller than 0.05.\n\n::: {.cell}\n\n```{.r .cell-code}\nsimulate_and_test <- function(){\n  x1 <- rnorm(10)\n  x2 <- rnorm(10) # same mean\n  ttest <- t.test(x1, x2)\n  ttest$p.value # we return the p-value\n}\n\nnull_p.values <- replicate(1000, simulate_and_test())\n```\n:::\n\nLet's count how many times we have a p-value smaller than 0.05 under the null hypothesis:\n\n::: {.cell}\n\n```{.r .cell-code}\nsum(null_p.values < 0.05)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 52\n```\n:::\n:::\n\nWhich corresponds to a rate of\n\n::: {.cell}\n\n```{.r .cell-code}\nmean(null_p.values < 0.05)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.052\n```\n:::\n:::\n\nwhich is, indeed, very close to the $p$-value threshold that we've selected.\n\nThis is because, under the null hypothesis, the distribution of $p$-values is uniform:\n\n::: {.cell}\n\n```{.r .cell-code}\nnull_p.values_tbl <- tibble(p_values = null_p.values)\n\nggplot(null_p.values_tbl, aes(x = p_values)) +\n  geom_histogram(bins = 20)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-40-1.png){width=672}\n:::\n:::\n\nThis is a good opportunity to learn how to perform a QQ-plot for another distribution than the Normal distribution. Here, to check that the p-values follow a uniform distribution, we can use the `distribution` argument from the `geom_qq` functions and do the following QQ-plot:\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(null_p.values_tbl, aes(sample = p_values)) +\n  geom_qq_line(distribution = qunif) +\n  geom_qq(distribution = qunif, size = 0.5, alpha = 0.5)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-41-1.png){width=672}\n:::\n:::\n\nThis is very convincing that the $p$-values under the null follow a uniform distribution.\n\nNow, let's come back to the interpretation/consequences of this uniform distribution: if the distribution of $p$-values under the Null is uniform, this means that we will reject the null hypothesis with a rate of $\\alpha$ if $\\alpha$ is the rejection threshold.\n\nThis will also apply to real data if we perform the same test several times.\n\nFor example, let's say that we were interested in not just IL-1$\\beta$ but all cytokines and are to repeat the $t$-test we did above for all cytokines, since we have 10 of them, it's not unlikely that we'd have small $p$-values just by chance.\n\nSo, whenever we do \"multiple testing\", we need to adjust for that multiple testing.\n\nThere are several ways to do \"multiple testing adjustments\" but the explanations of these methods are outside the scope of this class. Many of these methods have conveniently been implemented in the `p.adjust` function.\n\nOne of these methods that \"controls for the\"false discovery rate\" is the Benjamini-Hochberg adjustment.\n\nLet's apply it to our simulated $p$-values and check now how many samples are still considered to have a significant adjusted $p$-value:\n\n::: {.cell}\n\n```{.r .cell-code}\nnull_p.values_tbl <- \n  null_p.values_tbl |> \n  mutate(q_values = p.adjust(p_values, method = \"BH\"))\n\n\nmean(null_p.values_tbl$q_values <= 0.05)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0\n```\n:::\n:::\n\nNone of them!\n\nWhich is what it should be as we simulated our data under the null hypothesis, we should never reject the Null.\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(null_p.values_tbl, aes(x = q_values)) +\n  geom_histogram(bins = 20) +\n  expand_limits(x = 0) +\n  xlab(\"BH adjusted p-values\")\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-43-1.png){width=672}\n:::\n:::\n\n### Cytokines\n\nLet's now test which cytokines have different means in BV and non-BV study participants.\n\nFirst, we need to take the log-concentration of all cytokines and join with the clinical data via the \"sample info\".\n\n::: {.cell}\n\n```{.r .cell-code}\nelisa <- \n  elisa |> \n  mutate(logconc = log10(conc)) |> \n  left_join(sample_info, by = join_by(sample_id)) |> \n  left_join(clin, by = join_by(pid, time_point, arm))\n```\n:::\n\nWe can also display concentration by BV status for each cytokine:\n\n::: {.cell}\n\n```{.r .cell-code}\nelisa |> \n  ggplot(aes(x = logconc, fill = BV)) +\n  geom_histogram(bins = 30, position = \"identity\", alpha = 0.5) +\n  facet_wrap(cytokine ~ .) +\n  xlab('log10(concentrations)') +\n  scale_fill_manual(values = BV_colors)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-45-1.png){width=672}\n:::\n:::\n\nAnother way to look at this is to display the data as \"boxplot\":\n\n::: {.cell}\n\n```{.r .cell-code}\nelisa |> \n  ggplot(aes(y = logconc, fill = BV, col = BV, x = cytokine)) +\n  geom_boxplot(varwidth = TRUE, outlier.size = 0.5, alpha = 0.5) +\n  scale_fill_manual(values = BV_colors) +\n  scale_color_manual(values = BV_colors)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-46-1.png){width=672}\n:::\n:::\n\nLooking at these visual displays of the data. Do you think it makes sense to do the test for all cytokines? What about cytokines that have a lot of samples with concentrations lower than the limit of detection?\n\nShould we exclude these cytokines? Remove the out-of-range values? Or keep them?\n\nIdeally, we would only want to do the test for variables where we are confident that we have representative samples.\n\nThere is a little bit of subjectivity in terms of what we deem representative, but, for now, let's say that we want the 1st and 3rd quartile to be within the range of detection for each cytokine and each group.\n\nLet's thus compute the 1st and 3rd quartiles (or, equivalently the 25th and 75th percentiles) for each cytokines and each group:\n\n::: {.cell}\n\n```{.r .cell-code}\ninterquartiles <- \n  elisa |> \n  # we also need to compute the LLOQ and ULOQ for each cytokine\n  group_by(cytokine) |> \n  mutate(LLOQ = min(logconc), ULOQ = max(logconc)) |>\n  group_by(cytokine, LLOQ, ULOQ, BV) |> \n  summarize(\n    `1st quartile` = quantile(logconc, 0.25),\n    `3rd quartile` = quantile(logconc, 0.75)\n  ) \n```\n\n::: {.cell-output .cell-output-stderr}\n```\n`summarise()` has grouped output by 'cytokine', 'LLOQ', 'ULOQ'. You can\noverride using the `.groups` argument.\n```\n:::\n\n```{.r .cell-code}\ninterquartiles |> \n  pander()\n```\n\n::: {.cell-output-display}\n--------------------------------------------------------------------\n cytokine    LLOQ     ULOQ      BV      1st quartile   3rd quartile \n---------- --------- ------- --------- -------------- --------------\n  IFN-Y     -0.4641   1.369     BV        -0.4641        -0.1024    \n\n  IFN-Y     -0.4641   1.369   Healthy     -0.4641        -0.4641    \n\n  IL-10     -0.1152   1.482     BV        -0.1152         0.3345    \n\n  IL-10     -0.1152   1.482   Healthy     -0.1152        -0.1152    \n\n  IL-1a      0.143    3.011     BV         1.681          2.238     \n\n  IL-1a      0.143    3.011   Healthy      1.067          1.976     \n\n  IL-1b     -0.6012   2.347     BV         0.8021         1.334     \n\n  IL-1b     -0.6012   2.347   Healthy     -0.2347         0.9864    \n\n   IL-6     -1.029    2.238     BV        0.08762         0.7945    \n\n   IL-6     -1.029    2.238   Healthy      -0.364         0.6946    \n\n   IL-8     -0.3279   3.398     BV         2.164          2.916     \n\n   IL-8     -0.3279   3.398   Healthy      1.575           2.71     \n\n  IP-10     -0.5157   3.699     BV        -0.5157        0.06446    \n\n  IP-10     -0.5157   3.699   Healthy     -0.1079         1.945     \n\n   MIG      0.1872    4.398     BV         0.1872          1.07     \n\n   MIG      0.1872    4.398   Healthy      0.6586         2.023     \n\n  MIP-3a    0.7137    2.516     BV         0.7137         0.7137    \n\n  MIP-3a    0.7137    2.516   Healthy      0.7137         1.168     \n\n   TNFa     -0.327    1.631     BV         -0.327         0.5051    \n\n   TNFa     -0.327    1.631   Healthy      -0.327         0.1446    \n--------------------------------------------------------------------\n:::\n:::\n\nLet's filter for cytokines with an interquartile range within the limits of quantification for both groups:\n\n::: {.cell}\n\n```{.r .cell-code}\ninterquartiles |> \n  filter(\n    `1st quartile` > LLOQ,\n    `3rd quartile` < ULOQ\n  ) |> \n  group_by(cytokine) |>\n  summarize(n = n(), .groups = \"drop\") |> \n  filter(n == 2) |>\n  pander()\n```\n\n::: {.cell-output-display}\n--------------\n cytokine   n \n---------- ---\n  IL-1a     2 \n\n  IL-1b     2 \n\n   IL-6     2 \n\n   IL-8     2 \n--------------\n:::\n:::\n\nThat only leaves us with 4 cytokines. Our criteria might be a little too stringent, and, as long as you justify it properly, you could loosen these criteria. We just want to make sure that you always take a critical look at your data before running a test.\n\nWe will now filter our data to only keep these 4 cytokines and perform a statistical test for each of theses.\n\n::: {.cell}\n\n```{.r .cell-code}\nttest_res <- \n  elisa |> \n  group_by(cytokine) |> \n  filter(\n    cytokine %in% c(\"IL-1a\",\"IL-1b\", \"IL-6\", \"IL-8\")\n  ) |>\n  summarize(\n    `p-value` = t.test(logconc ~ BV)$p.value,\n    `p < 0.05` = ifelse(`p-value` < 0.05, \"yes\", \"\")\n  ) \n\nttest_res |> pander()\n```\n\n::: {.cell-output-display}\n---------------------------------\n cytokine    p-value    p < 0.05 \n---------- ----------- ----------\n  IL-1a     8.492e-08     yes    \n\n  IL-1b     1.672e-09     yes    \n\n   IL-6      0.08282             \n\n   IL-8      0.00825      yes    \n---------------------------------\n:::\n:::\n\nWe see that 3/4 cytokines have p-values smaller than 0.05. But remember, we need to adjust for multiple testing.\n\n::: {.cell}\n\n```{.r .cell-code}\nttest_res <- \n  ttest_res |> \n  mutate(\n    `adj. p-value` = p.adjust(`p-value`, method = \"BH\"),\n    `statistical\\nsignificance` = \n      ifelse(`adj. p-value` < 0.05, \"yes\",\"\")\n  )\n\n\nttest_res |> pander()\n```\n\n::: {.cell-output-display}\n---------------------------------------------------------------------------\n cytokine    p-value    p < 0.05   adj. p-value   statistical significance \n---------- ----------- ---------- -------------- --------------------------\n  IL-1a     8.492e-08     yes       1.698e-07               yes            \n\n  IL-1b     1.672e-09     yes       6.689e-09               yes            \n\n   IL-6      0.08282                 0.08282                               \n\n   IL-8      0.00825      yes         0.011                 yes            \n---------------------------------------------------------------------------\n:::\n:::\n\nThe three cytokines that had a p-value smaller than 0.05 are still significant after adjusting for multiple testing.\n\nRedo this analysis, but with a less stringent criteria for representativeness. For example, let's only request from our cytokines that their 1st and 3rd quartiles are different.\n\n::: {.cell}\n\n```{.r .cell-code}\nselected_cytokines <- \n  interquartiles |> \n  filter(\n    `1st quartile` < `3rd quartile`\n  ) |> \n  group_by(cytokine) |>\n  summarize(n = n(), .groups = \"drop\") |> \n  filter(n == 2) |>\n  select(cytokine) |> unlist()\n\nselected_cytokines\n```\n\n::: {.cell-output .cell-output-stdout}\n```\ncytokine1 cytokine2 cytokine3 cytokine4 cytokine5 cytokine6 cytokine7 \n  \"IL-1a\"   \"IL-1b\"    \"IL-6\"    \"IL-8\"   \"IP-10\"     \"MIG\"    \"TNFa\" \n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nttest_res <- \n  elisa |> \n  filter(cytokine %in% selected_cytokines) |> \n  group_by(cytokine) |>\n  summarize(\n    `p-value` = t.test(logconc ~ BV)$p.value,\n    `p < 0.05` = ifelse(`p-value` < 0.05, \"yes\", \"\")\n  )  |> \n  mutate(\n    `adj. p-value` = p.adjust(`p-value`, method = \"BH\"),\n    `statistical\\nsignificance` = \n      ifelse(`adj. p-value` < 0.05, \"yes\",\"\")\n  )\n\n\nttest_res |> pander()\n```\n\n::: {.cell-output-display}\n---------------------------------------------------------------------------\n cytokine    p-value    p < 0.05   adj. p-value   statistical significance \n---------- ----------- ---------- -------------- --------------------------\n  IL-1a     8.492e-08     yes       1.486e-07               yes            \n\n  IL-1b     1.672e-09     yes       5.853e-09               yes            \n\n   IL-6      0.08282                 0.08282                               \n\n   IL-8      0.00825      yes        0.01155                yes            \n\n  IP-10     1.974e-12     yes       1.381e-11               yes            \n\n   MIG      4.902e-08     yes       1.144e-07               yes            \n\n   TNFa      0.02701      yes        0.03151                yes            \n---------------------------------------------------------------------------\n:::\n:::\n\nWhat do you conclude from this analysis?\n\nAs an exercise, you can also re-do the analyses using a non-parametric test and discuss the differences in the results.\n\n## Displaying longitudinal data\n\nSo far, we have ignore the fact that we have longitudinal data.\n\nRemember the study design:\n\n![](images/study_design.png){fig-alt=\"The study design\" fig-align=\"center\" width=\"546\"}\n\nWe have 3 time points for each patient. We can thus look at the evolution of the cytokine concentrations over time.\n\nWe have already joined the `elisa` data with the `sample_info` data so we already have the participant id and the visit id for all cytokine samples:\n\n::: {.cell}\n\n```{.r .cell-code}\nelisa |>  head() |> pander()\n```\n\n::: {.cell-output-display}\n------------------------------------------------------------------------------\n sample_id   cytokine   conc       limits       logconc    pid     time_point \n----------- ---------- ------- --------------- --------- -------- ------------\n  SAMP094     IL-1a     173.9   within limits    2.24     pid_01    baseline  \n\n  SAMP094     IL-10     0.767   out of range    -0.1152   pid_01    baseline  \n\n  SAMP094     IL-1b     5.39    within limits   0.7316    pid_01    baseline  \n\n  SAMP094      IL-8     48.29   within limits    1.684    pid_01    baseline  \n\n  SAMP094      IL-6     5.07    within limits    0.705    pid_01    baseline  \n\n  SAMP094      TNFa     0.471   out of range    -0.327    pid_01    baseline  \n------------------------------------------------------------------------------\n\nTable: Table continues below\n\n \n-----------------------------------------------\n   arm     nugent_score   crp_blood   ph    BV \n--------- -------------- ----------- ----- ----\n placebo        8           0.44      5.7   BV \n\n placebo        8           0.44      5.7   BV \n\n placebo        8           0.44      5.7   BV \n\n placebo        8           0.44      5.7   BV \n\n placebo        8           0.44      5.7   BV \n\n placebo        8           0.44      5.7   BV \n-----------------------------------------------\n:::\n:::\n\nBefore jumping into the visualization, let's do some data wrangling.\n\nFirst, let's do some cosmetic changes to the `time_point` column so that the visits labels print nicely when using `ggplot`.\n\nTo replace the \"\\_\" with a space, we can use the `str_replace` function from the `stringr` package.\n\nBefore:\n\n::: {.cell}\n\n```{.r .cell-code}\nelisa$time_point |> unique()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"baseline\" \"week_1\"   \"week_7\"  \n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nelisa <- \n  elisa |> \n  mutate(time_point = time_point |> str_replace(\"_\", \" \"))\n```\n:::\n\nAfter:\n\n::: {.cell}\n\n```{.r .cell-code}\n# let's check that it worked\nelisa$time_point |> unique()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"baseline\" \"week 1\"   \"week 7\"  \n```\n:::\n:::\n\nThen, let's transform all the variables that are `character` but should be `factors` to ... `factor`.\n\n::: {.cell}\n\n```{.r .cell-code}\nelisa <- \n  elisa |> \n  mutate(\n    sample_id = sample_id |> factor(),\n    cytokine = cytokine |> factor(),\n    pid = pid |> factor(),\n    time_point =time_point |> factor(),\n    arm = arm |> factor(),\n    limits = limits |> factor()\n    )\n```\n:::\n\nNow, we can display the IL-1$\\beta$ log-concentrations over-time, connecting the concentrations of a given participant by a line.\n\nNote the use of the `group` aesthetic in `geom_line` to tell `ggplot` to connect the points of the same participant.\n\nCheck what happens if you remove `aes(group = pid)` from `geom_line`.\n\n::: {.cell}\n\n```{.r .cell-code}\narm_color <- \n  c(\"placebo\" = \"deeppink\", \"treatment\" = \"steelblue1\")\n\nelisa |> \n  filter(cytokine == \"IL-1b\") |> \n  ggplot(aes(x = time_point, y = logconc, color = arm)) +\n  geom_line(aes(group = pid), alpha = 0.75) +\n  geom_point(alpha = 0.5) +\n  xlab(\"Visits\") +\n  ylab(\"log10(IL-1b concentration)\") +\n  scale_color_manual(\"Arm\", values = arm_color)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-58-1.png){width=672}\n:::\n:::\n\n### Paired tests\n\nOne question that this study may have aimed to answer is whether treatment altered how the cytokine concentrations changed over time.\n\nSpecifically, we may have been interested in comparing the changes between the baseline visit and the week 7 visit.\n\nLet's do the same visualization, but excluding week 1:\n\n::: {.cell}\n\n```{.r .cell-code}\nelisa |> \n  filter(cytokine == \"IL-1b\", time_point != \"week 1\") |> \n  ggplot(aes(x = time_point, y = logconc, color = arm)) +\n  geom_line(aes(group = pid), alpha = 0.75) +\n  geom_point(alpha = 0.5) +\n  xlab(\"Visits\") +\n  ylab(\"log10(IL-1b concentration)\") +\n  scale_color_manual(\"Arm\", values = arm_color)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-59-1.png){width=672}\n:::\n:::\n\nWe see that everyone starts with a different level of IL-1b and the ranking in the baseline visit is somewhat similar to the ranking in the week 7 visit, especially in the treatment arm.\n\nTo test this specifically, we can do a \"paired test\".\n\nIt is \"paired\" because for each observation at the baseline visit, there is a \"sister\" observation at the week 7 visit (= the sample from the same participant).\n\nHere, because we have many samples, we can use a paired $t$-test, but if we had only one sample per participant, we would have to use a paired Wilcoxon test.\n\nIn a \"real\" analysis, we would only do one of the two tests, but for demonstration purposes, let's do both (without adjusting for multiple testing because it's just illustrative).\n\n::: {.cell}\n\n```{.r .cell-code}\nelisa |> \n  filter(cytokine == \"IL-1b\", time_point != \"week 1\") |> \n  select(cytokine, arm, pid, time_point, logconc) |>\n  pivot_wider(names_from = time_point, values_from = logconc) |>\n  group_by(arm) |> \n  summarize(\n    `p (t-test)` = \n      t.test(x = baseline, y = `week 7`, paired = TRUE)$p.value,\n    `p (signed rank)` = \n      wilcox.test(x = baseline, y = `week 7`, paired = TRUE)$p.value\n  ) |> \n  mutate(\n    `p < 0.05 (t)` = ifelse(`p (t-test)` < 0.05, \"yes\", \"\"),\n    `p < 0.05 (w)` = ifelse(`p (signed rank)` < 0.05, \"yes\", \"\")\n  ) |> \n  pander()\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning: There was 1 warning in `summarize()`.\nℹ In argument: `p (signed rank) = wilcox.test(x = baseline, y = `week 7`,\n  paired = TRUE)$p.value`.\nℹ In group 2: `arm = treatment`.\nCaused by warning in `wilcox.test.default()`:\n! cannot compute exact p-value with ties\n```\n:::\n\n::: {.cell-output-display}\n------------------------------------------------------------------------\n    arm      p (t-test)   p (signed rank)   p < 0.05 (t)   p < 0.05 (w) \n----------- ------------ ----------------- -------------- --------------\n  placebo      0.3998          0.41                                     \n\n treatment   0.0007528       0.001226           yes            yes      \n------------------------------------------------------------------------\n:::\n:::\n\nWe see that the two tests agree in this case (as they should because the effects are quite large in the treatment arm).\n\nExercise: repeat the same visualization and analysis for IL-6 and IL-8. Do you need to adjust for multiple testing?\n\n## Compositional data\n\nMultivariate data are said to be \"compositional\" when they are expressed in terms of *proportion* of a sample.\n\nThe problem with compositional data is that they are constrained to sum to 1, so that the values of the different variables are not independent.\n\nWhich means that if only one variable changes in absolute value (unknown), in relative values (known), the others will change too.\n\nFor example, if we have 3 cell sub-types: A, B, and C.\n\nIn one sample, we have 102 cells A, 72 cells B, and 147 cell C.\n\nIn code, we would write:\n\n::: {.cell}\n\n```{.r .cell-code}\nsample_1 <- c(\"A\" = 102, \"B\" = 72, \"C\" = 147)\nsample_1\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n  A   B   C \n102  72 147 \n```\n:::\n\n```{.r .cell-code}\n# note: usually we define vectors with just their values, like this: c(102, 72, 147), but we can also give names to each element of the vector as we did above\n```\n:::\n\nIn another sample, we have the same number of cells A and B, but we have 62 additional cells C.\n\n::: {.cell}\n\n```{.r .cell-code}\nsample_2 <- sample_1 \nsample_2[\"C\"] <- sample_2[\"C\"] + 62\n\n# the two lines above do the following: we create sample_2 as a copy of sample_1. Then, we only modify the \"C\" element of sample_2 and we add 62 to the value that was already there.\n```\n:::\n\nNow, we can compute the proportions of each cell type in each sample:\n\n::: {.cell}\n\n```{.r .cell-code}\nprop_1 <- sample_1 / sum(sample_1)\n\nprop_2 <- sample_2 / sum(sample_2)\n```\n:::\n\nAnd display them:\n\n::: {.cell}\n\n```{.r .cell-code}\nprop_1 |> round(digits = 2) # the `round` function rounds the number. The `digit` argument specifies the number of digits to keep after the decimal point. Try with digits = 0 or digits = 4 to see the difference in output.\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n   A    B    C \n0.32 0.22 0.46 \n```\n:::\n\n```{.r .cell-code}\nprop_2 |> round(digits = 2)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n   A    B    C \n0.27 0.19 0.55 \n```\n:::\n:::\n\nWe see that the proportion of C has increased (because there are more cells) and consequently, the proportions of A and B have decreased (even though there were the same number of cells).\n\n### Flow cytometry data\n\nFor now, let's explore the flow cytometry data.\n\n::: {.cell}\n\n```{.r .cell-code}\nflow <- \n  read_csv(\n    \"data/04_flow_cytometry_UKZN_workshop_2023.csv\"\n    )\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nRows: 132 Columns: 10\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (1): sample_id\ndbl (9): live_cd19_negative, cd45_negative, cd45_positive, neutrophils, non_...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n```\n:::\n:::\n\nWe have one row per sample. And remember, we have three sample per patient (one for each visit).\n\nOne of the first things we observe is that all columns (except `sample_id`) are (large) integer numbers. We say that we have \"count data\".\n\nRemember, that these are the number of cells observed in each \"gating\" categories. Also, remember that some cell categories are subset of other categories.\n\nFor example, \"live CD19-\" cells are roughly divided into \"CD45+\" and \"CD45-\" cells.\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(flow, \n       aes(y = live_cd19_negative, \n           x = cd45_positive + cd45_negative)\n       ) +\n  geom_abline(intercept = 0, slope = 1, col = \"purple1\") +\n  geom_point(alpha = 0.5) +\n  scale_x_log10() +\n  scale_y_log10()\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-66-1.png){width=672}\n:::\n:::\n\nAnd \"CD3+\" are roughly divided into \"CD4 T\" and \"CD8 T\" cells.\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(flow, \n       aes(x = cd3_positive, \n           y = cd4_t_cells + cd8_t_cells)\n       ) +\n  geom_abline(intercept = 0, slope = 1, col = \"purple1\") +\n  geom_point(alpha = 0.5) +\n  scale_x_log10() +\n  scale_y_log10()\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-67-1.png){width=672}\n:::\n:::\n\nSo, if we show the fraction of \"CD4 T\" cells against that of \"CD8 T\" cells among all T cells (the \"CD3+\" cells).\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(flow, \n       aes(x = cd4_t_cells/cd3_positive, \n           y = cd8_t_cells/cd3_positive )\n       ) +\n  geom_point(alpha = 0.5) +\n  xlab(\"CD4 T cells (fraction of all T cells)\") +\n  ylab(\"CD8 T cells (fraction of all T cells)\")\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-68-1.png){width=672}\n:::\n:::\n\nWe see that we have a negative correlation: when the proportion of CD8 T cells increases, the proportion of CD4 T cells decreases.\n\nWhat are the consequences of that?\n\nLet's say that we are interested in looking at the treatment effect on the proportion of different T cell sub-types. If we were to forget that these were compositional data, we could do the same as what we did for the cytokines, and perform a test for each cell sub-type.\n\nBut since we know that they are anti-correlated, if there is an effect on one sub-type, there might be an *opposite* effect on the other sub-type.\n\nLet's see if that's the case.\n\nFirst, let's create a simplified flow cytometry table with just the T cell data and compute the proportion of the CD4 and CD8 T cells.\n\n::: {.cell}\n\n```{.r .cell-code}\nflow_T <- \n  flow |> \n  select(sample_id, cd4_t_cells, cd8_t_cells, cd3_positive) |>\n  mutate(\n    cd4_t_cells_f = cd4_t_cells/cd3_positive,\n    cd8_t_cells_f = cd8_t_cells/cd3_positive,\n    sum = cd4_t_cells_f + cd8_t_cells_f,\n    remaining_t_cells_f = 1 - sum\n    ) \n\nhead(flow_T) |> pander()\n```\n\n::: {.cell-output-display}\n----------------------------------------------------------------------\n sample_id   cd4_t_cells   cd8_t_cells   cd3_positive   cd4_t_cells_f \n----------- ------------- ------------- -------------- ---------------\n  SAMP094        409           444           988            0.414     \n\n  SAMP052       7126          6009          15078          0.4726     \n\n  SAMP017       36212         13434         53758          0.6736     \n\n  SAMP046       2872          2629           6254          0.4592     \n\n  SAMP025       6384          3039          11355          0.5622     \n\n  SAMP050       2405          1310           4076           0.59      \n----------------------------------------------------------------------\n\nTable: Table continues below\n\n \n----------------------------------------------\n cd8_t_cells_f    sum     remaining_t_cells_f \n--------------- -------- ---------------------\n    0.4494       0.8634         0.1366        \n\n    0.3985       0.8711         0.1289        \n\n    0.2499       0.9235         0.07649       \n\n    0.4204       0.8796         0.1204        \n\n    0.2676       0.8299         0.1701        \n\n    0.3214       0.9114         0.08857       \n----------------------------------------------\n:::\n:::\n\nWe also computed the sum of the fractions of the CD4 and CD8 T cells and see that this sum is not always 1. That means that some cells are not CD4 or CD8 T cells. The distribution of the sum of their fraction goes as:\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(flow_T, aes(x = sum)) +\n  geom_histogram(bins = 30) +\n  geom_vline(xintercept = 1, col = \"red\", linetype = 2) +\n  xlab(\"sum of the fractions of CD4 and CD8 T cells\") \n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-70-1.png){width=672}\n:::\n:::\n\nAs said above, one question we may have is whether the proportions of CD4 and CD8 T cells are different between the two arms at the end of the intervention.\n\nTo be able to answer this question, we first need to join the flow cytometry data with the sample information (the `sample_info` table) so we know which sample is from which visit and which participant and their arm.\n\n::: {.cell}\n\n```{.r .cell-code}\nflow_T_with_info <- \n  flow_T |> \n  left_join(sample_info, by = \"sample_id\")\n\nflow_T_with_info |> head() |> pander()\n```\n\n::: {.cell-output-display}\n----------------------------------------------------------------------\n sample_id   cd4_t_cells   cd8_t_cells   cd3_positive   cd4_t_cells_f \n----------- ------------- ------------- -------------- ---------------\n  SAMP094        409           444           988            0.414     \n\n  SAMP052       7126          6009          15078          0.4726     \n\n  SAMP017       36212         13434         53758          0.6736     \n\n  SAMP046       2872          2629           6254          0.4592     \n\n  SAMP025       6384          3039          11355          0.5622     \n\n  SAMP050       2405          1310           4076           0.59      \n----------------------------------------------------------------------\n\nTable: Table continues below\n\n \n------------------------------------------------------------------------------\n cd8_t_cells_f    sum     remaining_t_cells_f    pid     time_point     arm   \n--------------- -------- --------------------- -------- ------------ ---------\n    0.4494       0.8634         0.1366          pid_01    baseline    placebo \n\n    0.3985       0.8711         0.1289          pid_01     week_1     placebo \n\n    0.2499       0.9235         0.07649         pid_01     week_7     placebo \n\n    0.4204       0.8796         0.1204          pid_02    baseline    placebo \n\n    0.2676       0.8299         0.1701          pid_02     week_1     placebo \n\n    0.3214       0.9114         0.08857         pid_02     week_7     placebo \n------------------------------------------------------------------------------\n:::\n:::\n\nLet's now display the proportions of these cells at the week 7 visit:\n\n::: {.cell}\n\n```{.r .cell-code}\nflow_T_with_info_long <- \n  flow_T_with_info |> \n  select(sample_id, pid, time_point, arm, ends_with(\"_f\")) |> \n  pivot_longer(\n    cols = ends_with(\"_f\"),\n    names_to = \"cell_type\",\n    values_to = \"fraction of T cells\"\n    )\n  \nflow_T_with_info_long |> \n  ggplot(aes(x = cell_type, y = `fraction of T cells`,\n             fill = arm)) +\n  geom_boxplot() +\n  scale_fill_manual(values = arm_color) \n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-72-1.png){width=672}\n:::\n:::\n\nIt looks like there might be some differences: a lower proportion of CD4 T cells in the intervention arm than in the placebo arm and and slightly higher proportions in the two others.\n\nBut from these compositional data, it is impossible to know if one of the cell type is driving these effects or if it is a combination of several changes in each cell types.\n\nOne could still do a test to see if the proportions are different in the two arms, but this is outside the scope of this workshop. Come back to the next ones for more :)\n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}